{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO416 - Machine Learning for Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 6 - Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = \"/vol/lab/course/416/data/unsupervised/\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the image viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "from utils.image_viewer import display_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation via clustering\n",
    "\n",
    "We can segment an image into consistent regions using unsupervised clustering techniques. This is different to semantic segmentation, as in clustering a segmented region does not have necessarily have a semantic meaning. However, clustering can still be useful as sometimes it is relatively easy to infer which region corresponds to which anatomical structure, as we will see.\n",
    "\n",
    "Let us start by loading an MRI brain scan `\"mri-brain.nii.gz\"`. For this scan, we already obtained a brain mask using a neuroimaging toolkit which allows us to mask out non-brain structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sitk.ReadImage(data_dir + 'mri-brain.nii.gz')\n",
    "msk = sitk.ReadImage(data_dir + 'mri-brain-mask.nii.gz')\n",
    "print('MR image')\n",
    "display_image(img, window=400, level=200)\n",
    "print('Brain mask')\n",
    "display_image(msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the brain mask to mask out non-brain regions by setting them to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = sitk.GetArrayFromImage(img)\n",
    "msk_array = sitk.GetArrayFromImage(msk)\n",
    "\n",
    "masked_array = img_array\n",
    "masked_array[msk_array==0] = 0\n",
    "\n",
    "img_masked = sitk.GetImageFromArray(masked_array)\n",
    "img_masked.CopyInformation(img)\n",
    "\n",
    "print('Masked image')\n",
    "display_image(img_masked, window=400, level=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation for clustering\n",
    "\n",
    "We want to run different clustering methods on the masked brain data. As the clustering techniques we will be using don't take any spatial information into account, we can simply flatten the 3D imaging data into one big 1D vector $X$.\n",
    "\n",
    "Let's visualise the data by plotting a normalised histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all non-zero voxels and flatten the data into a 1D numpy array\n",
    "X = img_array[msk_array > 0].flatten().reshape(-1, 1)\n",
    "\n",
    "# Get the number of points\n",
    "num_pts = len(X.flatten())\n",
    "\n",
    "# Extract the minimum and maximum intensity values and calculate the number of bins for the histogram\n",
    "lim_low = np.min(X).astype(np.int)\n",
    "lim_high = np.max(X).astype(np.int)\n",
    "num_bins = (lim_high - lim_low + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.hist(X, bins=num_bins, density=True, range=(lim_low, lim_high), color='lightgray');\n",
    "plt.xlim([0,350]); # we limit the x-axis to the range of interest\n",
    "plt.show()\n",
    "\n",
    "print('Number of points ' + str(num_pts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random subsampling\n",
    "\n",
    "We have about 1.4 million data points, which can make clustering computationally inefficient. A common strategy in these cases is to use a random subset of the original data.\n",
    "\n",
    "For example, we can randomly subsample the data with a given percentage, say 5%. The histogram is a bit noisier, but preserves the overall shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = 0.05\n",
    "X_subset = np.random.choice(X.flatten(),int(num_pts*sampling)).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.hist(X_subset, bins=num_bins, density=True, range=(lim_low, lim_high), color='lightgray');\n",
    "plt.xlim([0,350]);\n",
    "plt.show()\n",
    "\n",
    "print('Number of points ' + str(len(X_subset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many clusters do we want?\n",
    "\n",
    "That's the key question when applying clustering techniques. Sometimes we have a good idea about the number of clusters that we would like our data to be partitioned into. Sometimes we don't, and finding out the number of clusters is part of data exploration in unsupervised learning.\n",
    "\n",
    "For now, let's assume we know that our brain consists of mostly three tissue types, grey matter (GM), white matter (WM), and cerebrospinal fluid (CSF). So we set the number of clusters to three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means\n",
    "\n",
    "Let's start with one of the most popular and well known clustering techniques, k-means (as discussed in the lecture). `scikit-learn` provides us with an efficient implementation.\n",
    "\n",
    "**Task:** Go through the steps below, and try to understand what's happening in each step. You might want to check out the scikit-learn's [overview of clustering](http://scikit-learn.org/stable/modules/clustering.html) and the [k-means documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster\n",
    "\n",
    "# Create a k-means instance\n",
    "kmeans = cluster.KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Running k-means via the fit function\n",
    "kmeans.fit(X_subset)\n",
    "\n",
    "# Produce the clustering result for all image points\n",
    "y = kmeans.predict(img_array.flatten().reshape(-1, 1))\n",
    "\n",
    "# K-means will produce labels between 0 and (k-1), we want 0 to be background, so we shift the labels by one\n",
    "y = y + 1 # shift labels\n",
    "y[(msk_array == 0).flatten()] = 0 # zero background\n",
    "\n",
    "# Construct a 3D label map\n",
    "lab_array = y.reshape(img_array.shape).astype('uint8')\n",
    "seg_kmeans = sitk.GetImageFromArray(lab_array)\n",
    "seg_kmeans.CopyInformation(img)\n",
    "\n",
    "# Display the results using SimpleITK mapping of label maps to colours\n",
    "display_image(sitk.LabelToRGB(seg_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models\n",
    "\n",
    "There are many other clustering tecniques we can apply, and `scikit-learn` makes it very easy to swap between methods as they have a common interface based on `fit` and `predict` functions.\n",
    "\n",
    "**Task:** Try out the [GaussianMixture](http://scikit-learn.org/stable/modules/mixture.html#mixture) which we discussed in the lecture (slides 18 and following)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.mixture as mixture\n",
    "\n",
    "# Create a GaussianMixture instance\n",
    "gmm = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the GMM result\n",
    "\n",
    "The cell below will plot the GMM on top of the image histogram, similar to what you saw in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "def plot_gmm(x, gmm):\n",
    "    omega = gmm.weights_\n",
    "    mu = gmm.means_\n",
    "    sigma = np.sqrt(gmm.covariances_)\n",
    "    for ind in range(0,omega.shape[0]): \n",
    "        plt.plot(x,omega[ind]*mlab.normpdf(x, mu[ind], sigma[ind]), linewidth=2, label='GMM Component '+str(ind))\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.hist(X, bins=num_bins, density=True, range=(lim_low, lim_high), label='Intensity histogram', color='lightgray');\n",
    "x = np.linspace(lim_low,lim_high,num_bins).reshape(-1,1)\n",
    "plot_gmm(x,gmm)\n",
    "plt.plot(x,np.exp(gmm.score_samples(x)), linewidth=2, color='k', label='Gaussian Mixture Model')\n",
    "plt.xlim([0,350])\n",
    "plt.legend(loc=0, shadow=True, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other clustering methods\n",
    "\n",
    "**Optional Task:** Try out other [clustering methods](http://scikit-learn.org/stable/modules/clustering.html#clustering). For example, you can try out [BayesianGaussianMixture](http://scikit-learn.org/stable/modules/mixture.html#variational-bayesian-gaussian-mixture). You can also re-use the GMM plotting code from above to plot the results of the Bayesian GMM fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "For the MRI brain scan, we also have some reference segmentation of GM, WM, and CSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = sitk.ReadImage(data_dir + 'mri-brain-tissues.nii.gz')\n",
    "display_image(sitk.LabelToRGB(ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (a):** Devise a heuristic which allows you to identify which of the clusters obtained with a clustering method (e.g., k-means or GMMs) corresponds to which tissue class. In the reference segmentation we have labels as CSF=1, GM=2, and WM=3.\n",
    "\n",
    "**Task (b):** Using your heuristic, calculate the Dice Similarity Coefficients (DSC) for CSF, GM, and WM when comparing the clustering results to the reference segmentation. See the MLI-MIC-Summary notebook from the second coursework for how to compute DSC using SimpleITK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesion segmentation using clustering\n",
    "\n",
    "**Optional task:** Try out the above clustering methods with **different numbers of clusters** on the provided `\"ct-brain.nii.gz\"` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sitk.ReadImage(data_dir + 'ct-brain.nii.gz')\n",
    "msk = sitk.ReadImage(data_dir + 'ct-brain-mask.nii.gz')\n",
    "\n",
    "print('CT image')\n",
    "display_image(img, x=70, y=100, z=90, window=120, level=40)\n",
    "\n",
    "print('Brain mask')\n",
    "display_image(msk, x=70, y=100, z=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = sitk.GetArrayFromImage(img)\n",
    "msk_array = sitk.GetArrayFromImage(msk)\n",
    "\n",
    "masked_array = img_array\n",
    "masked_array[msk_array==0] = 0\n",
    "\n",
    "img_masked = sitk.GetImageFromArray(masked_array)\n",
    "img_masked.CopyInformation(img)\n",
    "\n",
    "print('Masked image')\n",
    "display_image(img_masked, x=70, y=100, z=90, window=120, level=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all non-zero voxels and flatten the data into a 1D numpy array\n",
    "X = img_array[msk_array > 0].flatten().reshape(-1, 1)\n",
    "\n",
    "# Get the number of points\n",
    "num_pts = len(X.flatten())\n",
    "\n",
    "# Extract the minimum and maximum intensity values and calculate the number of bins for the histogram\n",
    "lim_low = -20 # manually set intensity range of interest\n",
    "lim_high = 100 # manually set intensity range of interest\n",
    "num_bins = (lim_high - lim_low + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.hist(X, bins=num_bins, density=True, range=(lim_low,lim_high), color='lightgray');\n",
    "plt.xlim([0,80]) # we limit the x-axis to the range of interest\n",
    "plt.show()\n",
    "\n",
    "print('Number of points ' + str(num_pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = 0.05\n",
    "X_subset = np.random.choice(X.flatten(),int(num_pts*sampling)).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.hist(X_subset, bins=num_bins, density=True, range=(lim_low, lim_high), color='lightgray');\n",
    "plt.xlim([0,80]);\n",
    "plt.show()\n",
    "\n",
    "print('Number of points ' + str(len(X_subset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster\n",
    "\n",
    "# Create a k-means instance\n",
    "kmeans = cluster.KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Running k-means via the fit function\n",
    "kmeans.fit(X_subset)\n",
    "\n",
    "# Produce the clustering result for all image points\n",
    "y = kmeans.predict(img_array.flatten().reshape(-1, 1))\n",
    "\n",
    "# K-means will produce labels between 0 and (k-1), we want 0 to be background, so we shift the labels by one\n",
    "y = y + 1 # shift labels\n",
    "y[(msk_array == 0).flatten()] = 0 # zero background\n",
    "\n",
    "# Construct a 3D label map\n",
    "lab_array = y.reshape(img_array.shape).astype('uint8')\n",
    "seg_kmeans = sitk.GetImageFromArray(lab_array)\n",
    "seg_kmeans.CopyInformation(img)\n",
    "\n",
    "# Display the results using SimpleITK mapping of label maps to colours\n",
    "display_image(sitk.LabelToRGB(seg_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "As discussed in the unsupervised learning lecture, Principal Component Analysis (PCA) allows us to do dimensionality reduction and construct statistical models. The idea here is to use PCA to find the principal modes (or components) of largest variation in the data.\n",
    "\n",
    "This is illustrated below with a simple 2D toy example, similar to the one in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate 2D toy data, 1000 samples\n",
    "m = 1000\n",
    "x = np.random.normal(5,2,m)\n",
    "y = 2*x + np.random.normal(0,3,m)\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=100)\n",
    "plt.scatter(x,y,marker='.')\n",
    "plt.xlim([-10,20])\n",
    "plt.ylim([-5,25])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running PCA\n",
    "\n",
    "We now perform the steps of PCA as outlined in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Put the data together in one big matrix $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((x,y))\n",
    "n, m = X.shape\n",
    "print('Dimension:\\t' + str(n))\n",
    "print('Samples:\\t' + str(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_X = np.mean(X, axis=1)\n",
    "X_prime = (1 / np.sqrt(m-1)) * (X - np.tile(mu_X, (m, 1)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Run singular value decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, D, V = np.linalg.svd(np.matmul(X_prime,X_prime.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results by overlaying the principal components (directions of largest variations) with three times the standard deviation (capturing 99.7% of values around the mean of a normal distribution, see also [68–95–99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first principal component\n",
    "p1a = mu_X + U[:,0] * np.sqrt(D[0])*3;\n",
    "p1b = mu_X - U[:,0] * np.sqrt(D[0])*3;\n",
    "\n",
    "# second principal component\n",
    "p2a = mu_X + U[:,1] * np.sqrt(D[1])*3;\n",
    "p2b = mu_X - U[:,1] * np.sqrt(D[1])*3;\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=100)\n",
    "plt.scatter(X[0,:],X[1,:],marker='.', color='lightgray')\n",
    "plt.plot((p1a[0],p1b[0]),(p1a[1],p1b[1]), linewidth=3)\n",
    "plt.plot((p2a[0],p2b[0]),(p2a[1],p2b[1]), linewidth=3)\n",
    "plt.xlim([-10,20])\n",
    "plt.ylim([-5,25])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the Gaussian nature of PCA by overlaying a contour plot generating from a multivariate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "covar = np.matmul(X_prime,X_prime.T)\n",
    "gauss_2d = multivariate_normal(mu_X, covar)\n",
    "\n",
    "i, j = np.mgrid[-10:20:.01, -5:25:.01]\n",
    "pos = np.empty(i.shape + (2,))\n",
    "pos[:, :, 0] = i; pos[:, :, 1] = j\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=100)\n",
    "plt.scatter(X[0,:],X[1,:],marker='.', color='lightgray')\n",
    "plt.plot((p1a[0],p1b[0]),(p1a[1],p1b[1]), linewidth=3)\n",
    "plt.plot((p2a[0],p2b[0]),(p2a[1],p2b[1]), linewidth=3)\n",
    "plt.contour(i, j, gauss_2d.pdf(pos), cmap='viridis')\n",
    "plt.xlim([-10,20])\n",
    "plt.ylim([-5,25])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running PCA using scikit-learn\n",
    "\n",
    "We can also utilise the [PCA implementation](http://scikit-learn.org/stable/modules/decomposition.html#pca) of scikit-learn which is easier than doing it manually as above. It also has some additional features such as whitening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomp\n",
    "\n",
    "# Create PCA instance\n",
    "pca = decomp.PCA()\n",
    "\n",
    "# Fit the data\n",
    "pca.fit(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we get the same results. Note, scikit-learn's PCA will gives us the singular values which can be converted to eigenvalues $\\lambda_i=s_i^2 \\, / \\, (m-1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean from PCA\n",
    "mu_X = pca.mean_\n",
    "\n",
    "# Get principal modes (a.k.a. components) from PCA\n",
    "U = pca.components_.T\n",
    "\n",
    "# Get the eigenvalues from PCA's singular values\n",
    "D = pca.singular_values_**2 / (m - 1)\n",
    "\n",
    "gauss_2d = multivariate_normal(mu_X, pca.get_covariance())\n",
    "\n",
    "# first principal component\n",
    "p1a = mu_X + U[:,0] * np.sqrt(D[0])*3;\n",
    "p1b = mu_X - U[:,0] * np.sqrt(D[0])*3;\n",
    "\n",
    "# second principal component\n",
    "p2a = mu_X + U[:,1] * np.sqrt(D[1])*3;\n",
    "p2b = mu_X - U[:,1] * np.sqrt(D[1])*3;\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=100)\n",
    "plt.scatter(X[0,:],X[1,:],marker='.', color='lightgray')\n",
    "plt.plot((p1a[0],p1b[0]),(p1a[1],p1b[1]), linewidth=3)\n",
    "plt.plot((p2a[0],p2b[0]),(p2a[1],p2b[1]), linewidth=3)\n",
    "plt.contour(i, j, gauss_2d.pdf(pos), cmap='viridis')\n",
    "plt.xlim([-10,20])\n",
    "plt.ylim([-5,25])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Shape Models of the Spine\n",
    "\n",
    "Now, let's use PCA to construct a statistical shape model of the spine.\n",
    "\n",
    "We load the spine data consisting of 200 examples of 26 3D coordinates corresponding to the centroids of the vertebral bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "spine_data = np.genfromtxt('/vol/lab/course/416/data/unsupervised/spine-data.txt')\n",
    "n, m = spine_data.shape\n",
    "\n",
    "# the data is stored as a one 78x200 matrix\n",
    "# for visualisation purposes we split the data\n",
    "# into three sets of x, y, and z coordinates\n",
    "num_centroids = n//3;\n",
    "x_ind = range(num_centroids)\n",
    "y_ind = range(num_centroids,num_centroids*2)\n",
    "z_ind = range(num_centroids*2,num_centroids*3)\n",
    "\n",
    "cx = spine_data[x_ind,:];\n",
    "cy = spine_data[y_ind,:];\n",
    "cz = spine_data[z_ind,:];\n",
    "\n",
    "print('Dimension:\\t' + str(n))\n",
    "print('Samples:\\t' + str(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the raw input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_spines(x,y,z,max_range=None,marker_size=10,figure_size=5):\n",
    "\n",
    "    fig = plt.figure(figsize=(figure_size, figure_size), dpi=100)\n",
    "    ax = fig.gca(projection='3d')\n",
    "    for s in range(x.shape[1]):\n",
    "        ax.scatter(x[:,s], y[:,s], z[:,s], s=marker_size, marker='.')\n",
    "        ax.plot(x[:,s], y[:,s], z[:,s])\n",
    "\n",
    "    if max_range == None:\n",
    "        max_range = np.array([x.max()-x.min(), y.max()-y.min(), z.max()-z.min()]).max() / 2.0\n",
    "\n",
    "    mid_x = (x.max()+x.min()) * 0.5\n",
    "    mid_y = (y.max()+y.min()) * 0.5\n",
    "    mid_z = (z.max()+z.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "    ax.view_init(5,45)\n",
    "    ax.grid()\n",
    "    \n",
    "plot_spines(cx, cy, cz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice, the data is all over the place which is normal as the raw coordinates extracted from the original CT images are not in the same coordinate space. Let's spatially normalise the data a bit by centering all spine at their geometric mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial normalisation\n",
    "cx_norm = cx - np.tile(np.mean(cx,axis=0),(num_centroids,1))\n",
    "cy_norm = cy - np.tile(np.mean(cy,axis=0),(num_centroids,1))\n",
    "cz_norm = cz - np.tile(np.mean(cz,axis=0),(num_centroids,1))\n",
    "\n",
    "plot_spines(cx_norm, cy_norm, cz_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how the average human spine looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx_mean = np.mean(cx_norm,axis=1)\n",
    "cy_mean = np.mean(cy_norm,axis=1)\n",
    "cz_mean = np.mean(cz_norm,axis=1)\n",
    "\n",
    "plot_spines(cx_mean.reshape(-1,1), cy_mean.reshape(-1,1), cz_mean.reshape(-1,1), marker_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running PCA\n",
    "\n",
    "**Task:** Use scikit-learn's implementation to perform PCA on the normalised spine data. Make sure to assign the following variables correctly: `mu_X` (mean), `U` (eigenvectors), `D` eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as decomp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation\n",
    "\n",
    "Let's visualise the PCA shape model. We plot the mean shape and positive and negative deviations from the mean along the first three principal modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 3\n",
    "for i in range(num_modes):\n",
    "\n",
    "    # add and subtract 2 times the standard deviation from the mean\n",
    "    sp = mu_X + U[:,i] * np.sqrt(D[i]) * 3\n",
    "    sn = mu_X - U[:,i] * np.sqrt(D[i]) * 3\n",
    "    \n",
    "    cx = np.vstack((mu_X[x_ind], sp[x_ind], sn[x_ind])).T\n",
    "    cy = np.vstack((mu_X[y_ind], sp[y_ind], sn[y_ind])).T\n",
    "    cz = np.vstack((mu_X[z_ind], sp[z_ind], sn[z_ind])).T\n",
    "        \n",
    "    plot_spines(cx, cy, cz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive shape model viewer\n",
    "\n",
    "The interactive viewer below alows you to generate spines using our PCA shape model. Each slider controls the variation from the mean shape along the first six principal modes. Here, we allow variations of up to ten times the standard deviation which can result in quite extreme deformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "def plot_spine(mean_shape,modes,s1,s2,s3,s4,s5,s6):\n",
    "    spine = mu_X + U[:,0] * s1 + U[:,1] * s2 + U[:,2] * s3 + U[:,3] * s4 + U[:,4] * s5 + U[:,5] * s6\n",
    "    sx = spine[x_ind].reshape(-1,1)\n",
    "    sy = spine[y_ind].reshape(-1,1)\n",
    "    sz = spine[z_ind].reshape(-1,1)\n",
    "    plot_spines(sx, sy, sz, max_range=300, marker_size=100)\n",
    "\n",
    "def interactive_pca(mu_X,U,D):\n",
    "    interact(plot_spine,mean_shape=fixed(mu_X),modes=fixed(U),\n",
    "             **{'s%d' % (i+1): (-np.sqrt(D[i]) * 10, np.sqrt(D[i]) * 10, np.sqrt(D[i])) for i in range(6)});\n",
    "\n",
    "interactive_pca(mu_X,U,D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
